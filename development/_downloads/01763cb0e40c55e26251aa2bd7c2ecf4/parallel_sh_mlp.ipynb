{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Optimizing an MLP with Parallel SuccesiveHalving\n\nAn example for the usage of a model-free SuccessiveHalving intensifier in SMAC,\nfor parallel execution. The configurations are randomly sampled.\n\nThis examples uses a real-valued SuccessiveHalving through epochs.\n\n4 workers are allocated for this run. As soon as any worker is idle,\nSMAC internally creates more SuccessiveHalving instances to take\nadvantage of the idle resources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n\nimport numpy as np\nfrom ConfigSpace.hyperparameters import CategoricalHyperparameter, \\\n    UniformFloatHyperparameter, UniformIntegerHyperparameter\n\nfrom smac.configspace import ConfigurationSpace\nfrom smac.facade.roar_facade import ROAR\nfrom smac.scenario.scenario import Scenario\nfrom smac.intensification.successive_halving import SuccessiveHalving\nfrom smac.initial_design.random_configuration_design import RandomConfigurations\n\n# --------------------------------------------------------------\n# We need to provide a pickable function and use __main__\n# to be compliant with multiprocessing API\n# Below is a work around to have a packaged function called\n# mlp_from_cfg_func\n# --------------------------------------------------------------\nimport os\nimport sys\nsys.path.append(os.path.join(os.path.dirname(__file__)))\nfrom mlp_from_cfg_func import mlp_from_cfg  # noqa: E402\n# --------------------------------------------------------------\n\nif __name__ == '__main__':\n\n    logger = logging.getLogger(\"MLP-example\")\n    logging.basicConfig(level=logging.INFO)\n\n    # Build Configuration Space which defines all parameters and their ranges.\n    # To illustrate different parameter types,\n    # we use continuous, integer and categorical parameters.\n    cs = ConfigurationSpace()\n\n    # We can add multiple hyperparameters at once:\n    n_layer = UniformIntegerHyperparameter(\"n_layer\", 1, 4, default_value=1)\n    n_neurons = UniformIntegerHyperparameter(\"n_neurons\", 8, 512, log=True, default_value=10)\n    activation = CategoricalHyperparameter(\"activation\", ['logistic', 'tanh', 'relu'],\n                                           default_value='tanh')\n    batch_size = UniformIntegerHyperparameter('batch_size', 30, 300, default_value=200)\n    learning_rate_init = UniformFloatHyperparameter('learning_rate_init', 0.0001, 1.0, default_value=0.001, log=True)\n    cs.add_hyperparameters([n_layer, n_neurons, activation, batch_size, learning_rate_init])\n\n    # SMAC scenario object\n    scenario = Scenario({\"run_obj\": \"quality\",  # we optimize quality (alternative to runtime)\n                         \"wallclock-limit\": 100,  # max duration to run the optimization (in seconds)\n                         \"cs\": cs,  # configuration space\n                         \"deterministic\": \"true\",\n                         \"limit_resources\": True,  # Uses pynisher to limit memory and runtime\n                         # Alternatively, you can also disable this.\n                         # Then you should handle runtime and memory yourself in the TA\n                         \"cutoff\": 20,  # runtime limit for target algorithm\n                         \"memory_limit\": 3072,  # adapt this to reasonable value for your hardware\n                         })\n\n    # Intensification parameters\n    # Intensifier will allocate from 5 to a maximum of 25 epochs to each configuration\n    # Successive Halving child-instances are created to prevent idle\n    # workers.\n    intensifier_kwargs = {'initial_budget': 5, 'max_budget': 25, 'eta': 3,\n                          'min_chall': 1, 'instance_order': 'shuffle_once'}\n\n    # To optimize, we pass the function to the SMAC-object\n    smac = ROAR(scenario=scenario, rng=np.random.RandomState(42),\n                tae_runner=mlp_from_cfg,\n                intensifier=SuccessiveHalving,\n                intensifier_kwargs=intensifier_kwargs,\n                initial_design=RandomConfigurations,\n                n_jobs=4)\n\n    # Example call of the function with default values\n    # It returns: Status, Cost, Runtime, Additional Infos\n    def_value = smac.get_tae_runner().run(config=cs.get_default_configuration(),\n                                          instance='1', budget=25, seed=0)[1]\n    print(\"Value for default configuration: %.4f\" % def_value)\n\n    # Start optimization\n    try:\n        incumbent = smac.optimize()\n    finally:\n        incumbent = smac.solver.incumbent\n\n    inc_value = smac.get_tae_runner().run(config=incumbent, instance='1',\n                                          budget=25, seed=0)[1]\n    print(\"Optimized Value: %.4f\" % inc_value)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}