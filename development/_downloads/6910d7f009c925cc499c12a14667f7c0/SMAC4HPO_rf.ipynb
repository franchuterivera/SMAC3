{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using the SMAC-HPO interface to tune a random forest\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n\nimport numpy as np\nfrom ConfigSpace.hyperparameters import CategoricalHyperparameter, \\\n    UniformFloatHyperparameter, UniformIntegerHyperparameter\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\n\nfrom smac.configspace import ConfigurationSpace\nfrom smac.facade.smac_hpo_facade import SMAC4HPO\nfrom smac.scenario.scenario import Scenario\n\nboston = load_boston()\n\n\ndef rf_from_cfg(cfg, seed):\n    \"\"\"\n        Creates a random forest regressor from sklearn and fits the given data on it.\n        This is the function-call we try to optimize. Chosen values are stored in\n        the configuration (cfg).\n\n        Parameters:\n        -----------\n        cfg: Configuration\n            configuration chosen by smac\n        seed: int or RandomState\n            used to initialize the rf's random generator\n\n        Returns:\n        -----------\n        np.mean(rmses): float\n            mean of root mean square errors of random-forest test predictions\n            per cv-fold\n    \"\"\"\n    rfr = RandomForestRegressor(\n        n_estimators=cfg[\"num_trees\"],\n        criterion=cfg[\"criterion\"],\n        min_samples_split=cfg[\"min_samples_to_split\"],\n        min_samples_leaf=cfg[\"min_samples_in_leaf\"],\n        min_weight_fraction_leaf=cfg[\"min_weight_frac_leaf\"],\n        max_features=cfg[\"max_features\"],\n        max_leaf_nodes=cfg[\"max_leaf_nodes\"],\n        bootstrap=cfg[\"do_bootstrapping\"],\n        random_state=seed)\n\n    def rmse(y, y_pred):\n        return np.sqrt(np.mean((y_pred - y) ** 2))\n\n    # Creating root mean square error for sklearns crossvalidation\n    rmse_scorer = make_scorer(rmse, greater_is_better=False)\n    score = cross_val_score(rfr, boston.data, boston.target, cv=11, scoring=rmse_scorer)\n    return -1 * np.mean(score)  # Because cross_validation sign-flips the score\n\n\nlogger = logging.getLogger(\"RF-example\")\nlogging.basicConfig(level=logging.INFO)\n# logging.basicConfig(level=logging.DEBUG)  # Enable to show debug-output\nlogger.info(\"Running random forest example for SMAC. If you experience \"\n            \"difficulties, try to decrease the memory-limit.\")\n\n# Build Configuration Space which defines all parameters and their ranges.\n# To illustrate different parameter types,\n# we use continuous, integer and categorical parameters.\ncs = ConfigurationSpace()\n\n# We can add single hyperparameters:\ndo_bootstrapping = CategoricalHyperparameter(\n    \"do_bootstrapping\", [\"true\", \"false\"], default_value=\"true\")\ncs.add_hyperparameter(do_bootstrapping)\n\n# Or we can add multiple hyperparameters at once:\nnum_trees = UniformIntegerHyperparameter(\"num_trees\", 10, 50, default_value=10)\nmax_features = UniformIntegerHyperparameter(\"max_features\", 1, boston.data.shape[1], default_value=1)\nmin_weight_frac_leaf = UniformFloatHyperparameter(\"min_weight_frac_leaf\", 0.0, 0.5, default_value=0.0)\ncriterion = CategoricalHyperparameter(\"criterion\", [\"mse\", \"mae\"], default_value=\"mse\")\nmin_samples_to_split = UniformIntegerHyperparameter(\"min_samples_to_split\", 2, 20, default_value=2)\nmin_samples_in_leaf = UniformIntegerHyperparameter(\"min_samples_in_leaf\", 1, 20, default_value=1)\nmax_leaf_nodes = UniformIntegerHyperparameter(\"max_leaf_nodes\", 10, 1000, default_value=100)\n\ncs.add_hyperparameters([num_trees, min_weight_frac_leaf, criterion,\n                        max_features, min_samples_to_split, min_samples_in_leaf, max_leaf_nodes])\n\n# SMAC scenario object\nscenario = Scenario({\"run_obj\": \"quality\",  # we optimize quality (alternative runtime)\n                     \"runcount-limit\": 10,  # max. number of function evaluations; for this example set to a low number\n                     \"cs\": cs,  # configuration space\n                     \"deterministic\": \"true\",\n                     \"memory_limit\": 3072,  # adapt this to reasonable value for your hardware\n                     })\n\n# To optimize, we pass the function to the SMAC-object\nsmac = SMAC4HPO(scenario=scenario, rng=np.random.RandomState(42),\n                tae_runner=rf_from_cfg)\n\n# Example call of the function with default values\n# It returns: Status, Cost, Runtime, Additional Infos\ndef_value = smac.get_tae_runner().run(cs.get_default_configuration(), 1)[1]\nprint(\"Value for default configuration: %.2f\" % def_value)\n\n# Start optimization\ntry:\n    incumbent = smac.optimize()\nfinally:\n    incumbent = smac.solver.incumbent\n\ninc_value = smac.get_tae_runner().run(incumbent, 1)[1]\nprint(\"Optimized Value: %.2f\" % inc_value)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}